<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>强化学习 on 阿姆姆姆姆姆姆姆</title><link>https://amulil.github.io/categories/rl/</link><description>Recent content in 强化学习 on 阿姆姆姆姆姆姆姆</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Mon, 09 Jan 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://amulil.github.io/categories/rl/index.xml" rel="self" type="application/rss+xml"/><item><title>深度强化学习（DRL）算法 1</title><link>https://amulil.github.io/p/hello-world/</link><pubDate>Mon, 09 Jan 2023 00:00:00 +0000</pubDate><guid>https://amulil.github.io/p/hello-world/</guid><description>&lt;img src="https://amulil.github.io/p/hello-world/cover.jpg" alt="Featured image of post 深度强化学习（DRL）算法 1" />&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;blockquote>
&lt;p>“养成习惯的过程可以分为四个简单的步骤：提示、渴求、反应和奖励。”&lt;/p>
&lt;p>摘录来自《掌控习惯》詹姆斯·克利尔(James Clear) 此材料可能受版权保护。&lt;/p>
&lt;/blockquote>
&lt;h1 id="前言">前言&lt;/h1>
&lt;p>就像引言里所描述的养成习惯的四个步骤，如果我们想让机器也有自己的“习惯”，去掉机器没有的渴求属性，就是强化学习所做的事情 —— 帮机器养成“习惯”，而 DRL 就是使用深度学习的技术去实现强化学习算法。今天是系列文章的第一篇，会介绍最基础的 policy-based 的算法 —— REINFORCE。&lt;/p>
&lt;p>τ = {s0, a0, r1, s1, a1, r2, s2, a2 &amp;hellip; rT, sT, aT}, 那么根据 MDP 属性有:&lt;/p>
&lt;p>$$ p(\tau) = \prod_{t=0}^{T}p_{\theta}(a_{t}|s_{t})p(s_{t+1}|s_{t}, a_{t}) $$&lt;/p>
&lt;p>那么这里的 pθ 就是机器的策略，针对 s 会产生什么样的 a。因为当 s 和 a 都确定了，下一时刻的 s 是系统的固有属性，我们没办法控制，所以我们后面只关心 pθ 。
因为每一时刻的反应都会产生相应的回报，那么我们把每一时刻的回报加起来，就是这一系列反应产生的整体回报，我们用 R(τ) 表示。有了整体回报，我们就可以衡量机器行为的好坏。&lt;/p>
&lt;p>$$R(\tau) = \sum_{t=1}^{T}r_{t}$$&lt;/p>
&lt;p>机器应该更关心当下的回报还是未来的回报呢？我们引入系数 γ ，如果它是 0，我只关注当下，它的值越大，我们会越关心未来的回报。&lt;/p>
&lt;p>$$R_{\theta} = E_{\tau\sim p_{\theta}(\tau)}[R(\tau)] = \sum_{\tau}p_{\theta}(\tau)R(\tau)$$&lt;/p>
&lt;p>有了上面的信息，接下来我们只要使得机器的期望回报最大，我们的机器就会牛逼起来！&lt;/p>
&lt;p>$$R_{\theta} = E_{\tau\sim p_{\theta}(\tau)}[R(\tau)] = \sum_{\tau}p_{\theta}(\tau)R(\tau)$$&lt;/p>
&lt;h1 id="如何获得最大的期望回报">如何获得最大的期望回报？&lt;/h1>
&lt;p>由上面的期望回报的公式，我们可以通过调节策略增大可以获得更大回报的反应的概率，可以用深度神经网络表示策略，把 s 输入神经网络（可以是MLP\CNN\LSTM\Transformer&amp;hellip;）获得的输出就是每个反应的概率。s 输入到神经网络，得到 a，这里随机采样一个 a（为了探索，选最大的不一定是正确的） 然后得到下一时刻的 s，重复收集，就有了上面公式里的 τ，从而就积累了训练数据。那么有过机器学习经验的人，可以发现答案呼之欲出，就是求导，梯度下降是求最小值，那么求最大值就是加个负号。&lt;/p>
&lt;p>$$\theta \leftarrow \theta + \alpha \nabla \bar{R}(\theta)$$&lt;/p>
&lt;p>其中：&lt;/p>
&lt;p>$$
\begin{aligned}
\nabla \bar{R}(\theta) &amp;amp; =\nabla_\theta \sum_\tau p(\tau) R(\tau) \
&amp;amp; =\sum_\tau \nabla_\theta p(\tau) R(\tau) \
&amp;amp; =\sum_\tau p(\tau) \frac{\nabla_\theta p(\tau)}{p(\tau)} R(\tau) \quad\left(\text { 因为 } \nabla_\theta \log (z)=\frac{1}{z} \nabla_\theta z\right) \
&amp;amp; =\sum_\tau p(\tau) \nabla_\theta \log p(\tau) R(\tau) \
&amp;amp; \approx \frac{1}{m} \sum_{i=1}^m \nabla_\theta \log p\left(\tau^{(i)}\right) R\left(\tau^{(i)}\right) \
&amp;amp; =\frac{1}{m} \sum_{i=1}^m \sum_{t=1}^T \nabla_\theta \log p_\theta\left(a_t^{(i)} \mid s_t^{(i)}\right) R\left(\tau^{(i)}\right) \
&amp;amp; =\frac{1}{m} \sum_{i=1}^m R\left(\tau^{(i)}\right) \sum_{t=1}^T \nabla_\theta \log p_\theta\left(a_t^{(i)} \mid s_t^{(i)}\right)
\end{aligned}
$$&lt;/p>
&lt;h1 id="实现">实现&lt;/h1>
&lt;p>实现很简单，这里结合参考很快就可以看懂。&lt;/p>
&lt;h1 id="缺点">缺点&lt;/h1>
&lt;p>方法很简单，虽然说大道至简，但是用上述方法训练出来的机器还是不够牛逼。主要是因为方法里存在一些缺陷：&lt;/p>
&lt;ol>
&lt;li>策略产生的 τ 不能再次用于训练，因为策略一直在更新，策略更新了，那么更新之前产生的 τ 就得丢掉了，训练效率很低。&lt;/li>
&lt;li>τ 里的每个 a 用的都是同样的 R，更好的情况应该是每个 a 用一个 R。&lt;/li>
&lt;/ol>
&lt;h1 id="改进">改进&lt;/h1>
&lt;ul>
&lt;li>针对 1，下一篇文章介绍的 PPO 提出了改进&lt;/li>
&lt;li>针对 2：
我们可以这样设置 R（一种 Credit Assignment）：
$$R_{t}(\tau) = \sum_{t=t(a)}^{T}\gamma^{t-t(a)}r_{t},\gamma \in [0,1]$$&lt;/li>
&lt;/ul>
&lt;h1 id="参考">参考&lt;/h1>
&lt;ol>
&lt;li>深度强化学习——李宏毅 &lt;a class="link" href="https://www.bilibili.com/video/BV1MW411w79n/?spm_id_from=333.337.search-card.all.click&amp;amp;vd_source=880886f42f0b8f6ed75679ffe50b5384" target="_blank" rel="noopener"
>https://www.bilibili.com/video/BV1MW411w79n/?spm_id_from=333.337.search-card.all.click&amp;vd_source=880886f42f0b8f6ed75679ffe50b5384&lt;/a>&lt;/li>
&lt;li>Deep Reinforcement Learning: Pong from Pixels &lt;a class="link" href="http://karpathy.github.io/2016/05/31/rl/" target="_blank" rel="noopener"
>http://karpathy.github.io/2016/05/31/rl/&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py" target="_blank" rel="noopener"
>https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/unit5/unit5.ipynb" target="_blank" rel="noopener"
>https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/unit5/unit5.ipynb&lt;/a>&lt;/li>
&lt;li>What does log_prob do? &lt;a class="link" href="https://stackoverflow.com/questions/54635355/what-does-log-prob-do" target="_blank" rel="noopener"
>https://stackoverflow.com/questions/54635355/what-does-log-prob-do&lt;/a>&lt;/li>
&lt;/ol></description></item></channel></rss>